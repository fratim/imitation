eval_rl_policy:
    model: 0
    policy_prefix: sac
    named_configs:
        - ${environment.name}
    config_updates:
        policy_path: ${output_dir}/trained_rl_policies/${environment.name}/${sacred.eval_rl_policy.policy_prefix}/model${sacred.eval_rl_policy.model}
        policy_type: sac
        videos: 0
        seed: 0
        rollout_save_path: ${output_dir}/trained_rl_policies/${environment.name}/${sacred.eval_rl_policy.policy_prefix}/model${sacred.eval_rl_policy.model}/rollouts/final.pkl

train_adversarial:
    command_name: gail
    named_configs:
      - ${environment.name}
      - ${learner_enc.config}
      - ${exp_enc.config}
      - ${wandb.value}
    config_updates:
        rl:
            rl_algo: ${rl_algo}
        common:
            log_dir: logss/
            wandb:
                wandb_tag: ${task_name}
                wandb_name_prefix: ${task_name}
            slim_reduction: ${slim_reduction}
        demonstrations:
            rollout_path: ${output_dir}/trained_rl_policies/${demonstrator.name}/${dem_algo}/model${demonstrator.model}/rollouts/final.pkl
        seed: 0
        encoder_learner_kwargs:
            enc_is_linear: ${enc_is_linear}
            enc_use_bias: ${enc_use_bias}
            enc_scale: ${enc_scale}
        algorithm_kwargs:
            n_gen_updates_per_round: ${n_gen}
            n_disc_updates_per_round: ${n_disc}
            n_enc_updates_per_round: ${n_enc}
            enc_lr: ${enc_lr}
            disc_lr: ${disc_lr}
            use_wgan: ${use_wgan}
            disc_opt_cls: ${disc_opt_cls}
            alt_enc_disc: ${alt_enc_disc}
            actor_lr_half_steps: ${actor_lr_half_steps}
            scale_obs: ${scale_obs}
            n_expert_trajs: ${n_expert_trajs}
            encoder_batch_size: ${encoder_batch_size}
            use_airl: ${use_airl}

eval_irl_policy:
    named_configs:
        - ${environment.name}
    config_updates:
        policy_path: logss/checkpoints/final/gen_policy
        policy_type: ppo
        videos: 1
        seed: 0
