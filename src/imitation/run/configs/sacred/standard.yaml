eval_rl_policy:
    model: 0
    named_configs:
        - ${environment.name}
    config_updates:
        policy_path: ${output_dir}/trained_rl_policies/${environment.name}/${sacred.eval_rl_policy.config_updates.policy_type}/model${sacred.eval_rl_policy.model}
        policy_type: sac
        videos: 0
        seed: 999
        rollout_save_path: ${output_dir}/trained_rl_policies/${environment.name}/${sacred.eval_rl_policy.config_updates.policy_type}/model${sacred.eval_rl_policy.model}/rollouts/final.pkl

train_adversarial:
    command_name: gail
    named_configs:
      - ${environment.name}
      - ${learner_enc.config}
      - ${exp_enc.config}
      - ${wandb.value}
    config_updates:
        rl:
            rl_algo: ${rl_algo}
        common:
            log_dir: logss/
            wandb:
                wandb_tag: ${task_name}
                wandb_name_prefix: ${task_name}
        demonstrations:
            rollout_path: ${output_dir}/trained_rl_policies/${environment.name}/${dem_algo}/model${demonstrator.model}/rollouts/final.pkl
        seed: 999
        algorithm_kwargs:
            n_gen_updates_per_round: ${n_gen}
            n_disc_updates_per_round: ${n_disc}
            n_enc_updates_per_round: ${n_enc}
            enc_lr: ${enc_lr}
            disc_lr: ${disc_lr}
            use_wgan: ${use_wgan}
            disc_opt_cls: ${disc_opt_cls}
            alt_enc_disc: ${alt_enc_disc}
            actor_lr_half_steps: ${actor_lr_half_steps}
            scale_obs: ${scale_obs}

eval_irl_policy:
    named_configs:
        - ${environment.name}
    config_updates:
        policy_path: logss/checkpoints/final/gen_policy
        policy_type: ppo
        videos: 1
        seed: 999
