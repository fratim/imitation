train_rl:
    named_configs: ${run_test.train_rl.named_configs}
    config_updates:
        common:
            log_dir: logss/
        seed: ${seed.value}

eval_rl_policy:
    named_configs:
        - ${env.name}
    config_updates:
        policy_path: logss/policies/final
        policy_type: ppo
        videos: 1
        seed: ${seed.value}


eval_dimensions:
    named_configs: ${run_test.train_adversarial.named_configs}
    config_updates:
        common:
            log_dir: logss/
        demonstrations:
            rollout_path_good: /Users/tim/Code/imitation/rl_demos/good_runs/env=seals_mountain_car_org,seed=one/logss/rollouts/final.pkl
            rollout_path_bad: /Users/tim/Code/imitation/rl_demos/low_train_runs/env=seals_mountain_car_org,run_test=minimum_training/logss/rollouts/initial.pkl
        seed: ${seed.value}

train_adversarial:
    command_name: ${irl_algo.name}
    named_configs: ${run_test.train_adversarial.named_configs}
    config_updates:
        common:
            log_dir: logss/
        demonstrations:
            rollout_path: /work/frtim/airl_saved/rl_demos/${demonstrator.name}/logss/rollouts/final.pkl
        seed: ${seed.value}
        reward:
            algorithm_specific:
                gail:
                    net_kwargs:
                        use_state: ${state_only.use_state}
                        use_action: ${state_only.use_action}
                        use_next_state: ${state_only.use_next_state}

eval_irl_policy:
    named_configs:
        - ${env.name}
    config_updates:
        policy_path: logss/checkpoints/final/gen_policy
        policy_type: ppo
        videos: 1
        seed: ${seed.value}
