train_rl:
    named_configs: ${run_test.train_rl.named_configs}
    config_updates:
        common:
            log_dir: logss/
        seed: ${seed.value}

eval_rl_policy:
    named_configs:
        - ${env.name}
    config_updates:
        policy_path: logss/policies/final
        policy_type: ppo
        videos: 1
        seed: ${seed.value}

train_adversarial:
    command_name: ${irl_algo.name}
    named_configs: ${run_test.train_adversarial.named_configs}
    config_updates:
        common:
            log_dir: logss/
        demonstrations:
            rollout_path: ${exec.output_dir}/airl_saved/rl_demos/${demonstrator.name}/logss/rollouts/final.pkl
        seed: ${seed.value}
        reward:
            algorithm_specific:
                gail:
                    net_kwargs:
                        use_state: ${state_only.use_state}
                        use_action: ${state_only.use_action}
                        use_next_state: ${state_only.use_next_state}
        encoder:
            use_encoder: ${use_encoder.value}
        invert_states_expert: ${invert_states_expert.value}

eval_irl_policy:
    named_configs:
        - ${env.name}
    config_updates:
        policy_path: logss/checkpoints/final/gen_policy
        policy_type: ppo
        videos: 1
        seed: ${seed.value}
